# Portainer Fresh Deploy - Quick Guide

## üéØ Quick Answer: How to Wipe Data and Re-scrape

### Method 1: Delete Volume in Portainer (Easiest)

**In Portainer UI:**

1. **Stop Stack:** Stacks ‚Üí `blucatch` ‚Üí Stop
2. **Delete Volume:** Volumes ‚Üí `blucatch_data` ‚Üí Remove
3. **Redeploy:** Stacks ‚Üí `blucatch` ‚Üí Deploy the stack

**Result:** Fresh database, full re-scrape on startup

---

### Method 2: Set Environment Variable (No Volume Delete)

**In Portainer Stack Editor:**

Add/update environment variables:

```yaml
services:
    blucatch-scraper:
        environment:
            - FORCE_FRESH=true # ‚Üê Add this to wipe data
            - SCRAPE_MODE=full # full, routes-only, or pokemon-only
            - START_POKEMON=1
            - END_POKEMON=151 # Or 1025 for all Pokemon
            - CRON_SCHEDULE=30 3 * * *
```

**Click "Update the stack"**

**Result:** Next deploy will:

-   Backup existing database
-   Clear locations and cache tables
-   Run full scrape with enhanced data

---

### Method 3: Command Line (SSH into server)

```bash
# Stop stack
docker-compose down

# Delete volume
docker volume rm blucatch_data

# Rebuild and start
docker-compose up -d --build

# Monitor scraper
docker logs -f blucatch-scraper
```

---

## üîß Environment Variables

| Variable        | Default       | Description                              |
| --------------- | ------------- | ---------------------------------------- |
| `FORCE_FRESH`   | `false`       | If `true`, wipes data on startup         |
| `SCRAPE_MODE`   | `routes-only` | `full`, `routes-only`, or `pokemon-only` |
| `START_POKEMON` | `1`           | First Pokemon ID to scrape               |
| `END_POKEMON`   | `151`         | Last Pokemon ID to scrape                |
| `CRON_SCHEDULE` | `30 3 * * *`  | Daily at 3:30 AM                         |

### Examples:

**Full Gen 1 scrape on every deploy:**

```yaml
environment:
    - FORCE_FRESH=true
    - SCRAPE_MODE=full
    - START_POKEMON=1
    - END_POKEMON=151
```

**Incremental updates (default):**

```yaml
environment:
    - FORCE_FRESH=false
    - SCRAPE_MODE=routes-only
```

**Full Pok√©dex scrape:**

```yaml
environment:
    - FORCE_FRESH=true
    - SCRAPE_MODE=full
    - START_POKEMON=1
    - END_POKEMON=1025
```

---

## üîç Monitor Deployment

### Check Scraper Logs

**In Portainer:**

-   Containers ‚Üí `blucatch-scraper` ‚Üí Logs

**Via CLI:**

```bash
# Live logs
docker logs -f blucatch-scraper

# Initial scrape log
docker exec blucatch-scraper cat /var/log/initial-scrape.log

# Cron log
docker exec blucatch-scraper cat /var/log/cron.log
```

### Check Database Stats

```bash
# Enhanced encounters count
docker exec blucatch-scraper sqlite3 /app/public/data/pokemon.db "
SELECT COUNT(*) FROM encounters WHERE location_id IS NOT NULL;
"

# Cached routes count
docker exec blucatch-scraper sqlite3 /app/public/data/pokemon.db "
SELECT COUNT(*) FROM scraper_cache WHERE status = 'complete';
"

# Last scrape time
docker exec blucatch-scraper sqlite3 /app/public/data/pokemon.db "
SELECT datetime(MAX(last_queried_at), 'unixepoch') as last_scrape
FROM scraper_cache;
"
```

---

## üöÄ Recommended Workflow

### For Fresh Production Deploy:

1. **Set in Portainer Stack:**

    ```yaml
    environment:
        - FORCE_FRESH=true
        - SCRAPE_MODE=full
        - START_POKEMON=1
        - END_POKEMON=151 # Gen 1 first
    ```

2. **Deploy and Monitor:**

    ```bash
    docker logs -f blucatch-scraper
    ```

3. **After Initial Scrape Completes:**
    - Change `FORCE_FRESH=false`
    - Change `SCRAPE_MODE=routes-only`
    - Update stack (incremental updates from now on)

### For Weekly Fresh Scrapes:

Keep `FORCE_FRESH=true` permanently, or use the auto-stale detection (>30 days).

---

## üìã What Happens on Startup

The new `docker-entrypoint.sh` script:

1. ‚úÖ Checks if `FORCE_FRESH=true` ‚Üí Wipes route data
2. ‚úÖ Checks if database exists ‚Üí Creates if missing
3. ‚úÖ Enhances schema (locations, cache tables)
4. ‚úÖ Splits concatenated location strings
5. ‚úÖ Checks data age ‚Üí Auto-triggers full scrape if >30 days
6. ‚úÖ Runs configured scrape mode
7. ‚úÖ Generates API endpoints
8. ‚úÖ Sets up cron for daily updates
9. ‚úÖ Displays stats

---

## ‚ú® Features

**Smart Initialization:**

-   Detects stale data (>30 days) ‚Üí Auto full re-scrape
-   Creates database if missing
-   Backs up before wiping

**Flexible Modes:**

-   `full` - Complete scrape (Pokemon pages + routes)
-   `routes-only` - Fast updates (uses cache)
-   `pokemon-only` - Build location database

**Resume Support:**

-   Cache tracks progress
-   Can stop and restart anytime
-   No duplicate work

---

## üîê Production Best Practices

### Initial Deploy:

```yaml
FORCE_FRESH=true
SCRAPE_MODE=full
START_POKEMON=1
END_POKEMON=151
```

### After Initial Scrape:

```yaml
FORCE_FRESH=false
SCRAPE_MODE=routes-only # Daily incremental updates
```

### Monthly Full Refresh:

Use auto-stale detection (>30 days) or manually set:

```yaml
FORCE_FRESH=true
SCRAPE_MODE=full
START_POKEMON=1
END_POKEMON=1025 # All Pokemon
```

---

## üìù Quick Commands

```bash
# Fresh deploy (wipe everything)
docker-compose down && docker volume rm blucatch_data && docker-compose up -d --build

# Check if data is fresh
docker exec blucatch-scraper sqlite3 /app/public/data/pokemon.db "
SELECT
  (SELECT COUNT(*) FROM encounters WHERE location_id IS NOT NULL) as enhanced,
  (SELECT COUNT(*) FROM scraper_cache WHERE status = 'complete') as cached_routes,
  datetime(MAX(last_queried_at), 'unixepoch') as last_scrape
FROM scraper_cache;
"

# Force re-scrape without stopping
docker exec blucatch-scraper node /app/scripts/scraper-main.js --mode full --force

# View progress
docker logs -f blucatch-scraper
```

---

## üéâ Summary

**To guarantee fresh data on every redeploy:**

Set in Portainer stack:

```yaml
environment:
    - FORCE_FRESH=true
```

**Or delete the volume before redeploying:**

-   Portainer ‚Üí Volumes ‚Üí `blucatch_data` ‚Üí Remove

That's it! The scraper will handle everything else automatically. üöÄ
